{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Wuzzuf\n",
    "\n",
    "> In this notebook you will scrape wuzzuf for the job title you need and you will get some of the latest info \n",
    "> about this job title.\n",
    "> \n",
    "> To get the full experience check out the __[github repo](https://github.com/AbdulrahmanYaseen/web-scrape-wuzzuf-for-a-job-title-and-get-a-dashboard-out-of-the-data)__ and download the power bi file and get a dashboard for your scraped data!\n",
    ">\n",
    "> In the last cell make sure your lists have the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter required data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the job you want to search for\n",
    "job = \"DaTa analyst\"\n",
    "driver_path = f\"C:\\\\webdrivers\\\\chromedriver.exe\"\n",
    "file_path = \"E:\\\\Webscrapingwuzzuf\\\\Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = job.strip().lower().replace(\" \", \"%20\")\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create needed lists\n",
    "#these ones for the data within the jobssearch page without entering each job's link\n",
    "job_title = []\n",
    "company_name = []\n",
    "location = []\n",
    "links = []\n",
    "date = []\n",
    "\n",
    "# these ones for each job's link\n",
    "Salary = []\n",
    "Experience_Needed = []\n",
    "Career_Level = []\n",
    "Education_Level = []\n",
    "Job_Categories = []\n",
    "No_of_applicants = []\n",
    "Job_Requirements = []\n",
    "\n",
    "# using requeset & beautifulsoup to scrape needed information\n",
    "page_num = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        #getting the links for each job iterating through each page\n",
    "        result = requests.get(f\"https://wuzzuf.net/search/jobs/?a=navbl%7Cspbl&q={job}&start={page_num}\")\n",
    "        src = result.content\n",
    "        soup = BeautifulSoup(src, \"lxml\")\n",
    "        page_limit = int(soup.find(\"strong\").text)\n",
    "        \n",
    "        #the page limit is the number of jobs\n",
    "        #so wewant to stop after reaching the last page\n",
    "        if (page_num > page_limit // 15):\n",
    "            print(\"Pages ended\")\n",
    "            break\n",
    "            \n",
    "        #getting needed info\n",
    "        job_titles = soup.find_all(\"h2\", {\"class\": \"css-m604qf\"})\n",
    "        company_names = soup.find_all(\"a\", {\"class\": \"css-17s97q8\"})\n",
    "        location_names = soup.find_all(\"span\", {\"class\": \"css-5wys0k\"})\n",
    "        \n",
    "        #posting date divs differ so\n",
    "        posted_new = soup.find_all(\"div\", {\"class\": \"css-4c4ojb\"})\n",
    "        posted_old = soup.find_all(\"div\", {\"class\": \"css-do6t5g\"})\n",
    "        posted = [*posted_new, *posted_old]\n",
    "        \n",
    "        #append data to our lists\n",
    "        for i in range(len(job_titles)):\n",
    "            job_title.append(job_titles[i].text)\n",
    "            links.append(job_titles[i].find(\"a\").attrs['href'])\n",
    "            company_name.append(company_names[i].text)\n",
    "            location.append(location_names[i].text)\n",
    "            date_text = posted[i].text.replace(\"_\", \"\").strip()\n",
    "            date.append(date_text)\n",
    "\n",
    "        page_num += 1\n",
    "        print(\"Page Switched\")\n",
    "\n",
    "    except:\n",
    "        print(\"error occurred\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using selenium to iterate through each link and get needed info\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        #getting each info by it's x_path\n",
    "        driver.get(link)\n",
    "        \n",
    "        No_applicants = driver.find_element_by_xpath('//strong[@class = \"css-u1gwks\"]')\n",
    "        No_of_applicants.append(No_applicants.text)\n",
    "        \n",
    "        experiance = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][1]/span[2]/span')\n",
    "        Experience_Needed.append(experiance.text)\n",
    "        \n",
    "        careerlevel = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][2]/span[2]/span')\n",
    "        Career_Level.append(careerlevel.text)\n",
    "        \n",
    "        education = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][3]/span[2]/span')\n",
    "        Education_Level.append(education.text)\n",
    "        \n",
    "        #Job_Categories is more than one so we will loop through the elements of it's x_path\n",
    "        Jobcategories_text = \"\"\n",
    "        Jobcategories = driver.find_elements_by_xpath('//div[@class=\"css-13sf2ik\"]/ul/li/a/span')\n",
    "        for jobcat in Jobcategories:\n",
    "            Jobcategories_text += jobcat.text + \" | \"\n",
    "        Jobcategories_text  = Jobcategories_text[:-2]\n",
    "        Job_Categories.append(Jobcategories_text)\n",
    "        \n",
    "        #Requirements is more than one so we will loop through the elements of it's x_path\n",
    "        reqs_text = \"\"\n",
    "        reqs = driver.find_elements_by_xpath('//div[@class = \"css-1t5f0fr\"]/ul/li')\n",
    "        for req in reqs:\n",
    "            reqs_text += req.text + \" | \"\n",
    "        reqs_text = reqs_text[:-2]\n",
    "        Job_Requirements.append(reqs_text)\n",
    "            \n",
    "        salaries = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][4]/span[2]/span')\n",
    "        Salary.append(salaries.text)\n",
    "    except:\n",
    "        Experience_Needed.append(\"not found\")\n",
    "        Career_Level.append(\"not found\")\n",
    "        Education_Level.append(\"not found\")\n",
    "        Job_Categories.append(\"not found\")        \n",
    "        Salary.append(\"not found\")\n",
    "        print(\"one job data not found\")\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv file\n",
    "\n",
    "\n",
    "file_list = [job_title, company_name, location, date, Salary, \n",
    "             Experience_Needed, Career_Level, Education_Level, Job_Categories, links, No_of_applicants, Job_Requirements]\n",
    "exported = zip_longest(*file_list)\n",
    "\n",
    "with open(file_path, \"w\", encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(['job_title', 'company_name', 'location', 'date', 'Salary', 'Experience_Needed', 'Career_Level',\n",
    "                 'Education_Level', 'Job_Categories', 'links', 'No_of_applicants', 'Job_Requirements'])\n",
    "    wr.writerows(exported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all lists have the same length\n",
    "file_list = [job_title, company_name, location, date, Salary, \n",
    "             Experience_Needed, Career_Level, Education_Level, Job_Categories, links, No_of_applicants, Job_Requirements]\n",
    "for i in (file_list):\n",
    "    print(len(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
