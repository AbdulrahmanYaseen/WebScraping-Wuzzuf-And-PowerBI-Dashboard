{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping Wuzzuf\n",
    "\n",
    "> In this notebook you will scrape wuzzuf for the job title you need and you will get some of the latest info \n",
    "> about this job title.\n",
    "> \n",
    "> To get the full experience check out the __[github repo](https://github.com/AbdulrahmanYaseen/web-scrape-wuzzuf-for-a-job-title-and-get-a-dashboard-out-of-the-data)__ and download the power bi file and get a dashboard for your scraped data!\n",
    ">\n",
    "> In the last cell make sure your lists have the same length"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enter required data here:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#enter the job you want to search for\r\n",
    "job = \"DaTa analyst\"\r\n",
    "filters = {\r\n",
    "    'career_level': ['entry level'],\r\n",
    "    'job_types': [],\r\n",
    "    'post_date': '24 hours'\r\n",
    "}\r\n",
    "driver_path = \"comp/chromedriver.exe\"\r\n",
    "file_path = \"comp/Data.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run all the following cells:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import needed libraries\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import csv\r\n",
    "from itertools import zip_longest\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.common.keys import Keys\r\n",
    "from selenium.webdriver.common.by import By\r\n",
    "from selenium.webdriver.chrome.options import Options\r\n",
    "import time\r\n",
    "from url_builder import url_gen\r\n",
    "import lxml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "url = url_gen(job, filters)\r\n",
    "url"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://wuzzuf.net/search/jobs/?a=navbl&filters%5Bcareer_level%5D%5B0%5D=Entry%20Level&filters%5Bjob_types%5D%5B0%5D=full_time&filters%5Bjob_types%5D%5B1%5D=work_from_home&filters%5Bpost_date%5D%5B0%5D=within_24_hours&q=data%20analyst&start=0'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create needed lists\r\n",
    "#these ones for the data within the jobssearch page without entering each job's link\r\n",
    "job_title = []\r\n",
    "company_name = []\r\n",
    "location = []\r\n",
    "links = []\r\n",
    "date = []\r\n",
    "\r\n",
    "# these ones for each job's link\r\n",
    "Salary = []\r\n",
    "Experience_Needed = []\r\n",
    "Career_Level = []\r\n",
    "Education_Level = []\r\n",
    "Job_Categories = []\r\n",
    "No_of_applicants = []\r\n",
    "Job_Requirements = []\r\n",
    "\r\n",
    "# using requeset & beautifulsoup to scrape needed information\r\n",
    "page_num = 0\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        #getting the links for each job iterating through each page\r\n",
    "        url = url[:-1] + str(page_num)\r\n",
    "        result = requests.get(url)\r\n",
    "        src = result.content\r\n",
    "        soup = BeautifulSoup(src, \"lxml\")\r\n",
    "        page_limit = int(soup.find(\"strong\").text)\r\n",
    "        \r\n",
    "        #the page limit is the number of jobs\r\n",
    "        #so wewant to stop after reaching the last page\r\n",
    "        if (page_num > page_limit // 15):\r\n",
    "            print(\"Pages ended\")\r\n",
    "            break\r\n",
    "            \r\n",
    "        #getting needed info\r\n",
    "        job_titles = soup.find_all(\"h2\", {\"class\": \"css-m604qf\"})\r\n",
    "        company_names = soup.find_all(\"a\", {\"class\": \"css-17s97q8\"})\r\n",
    "        location_names = soup.find_all(\"span\", {\"class\": \"css-5wys0k\"})\r\n",
    "        \r\n",
    "        #posting date divs differ so\r\n",
    "        posted_new = soup.find_all(\"div\", {\"class\": \"css-4c4ojb\"})\r\n",
    "        posted_old = soup.find_all(\"div\", {\"class\": \"css-do6t5g\"})\r\n",
    "        posted = [*posted_new, *posted_old]\r\n",
    "        \r\n",
    "        #append data to our lists\r\n",
    "        for i in range(len(job_titles)):\r\n",
    "            job_title.append(job_titles[i].text)\r\n",
    "            links.append(job_titles[i].find(\"a\").attrs['href'])\r\n",
    "            company_name.append(company_names[i].text)\r\n",
    "            location.append(location_names[i].text)\r\n",
    "            date_text = posted[i].text.replace(\"_\", \"\").strip()\r\n",
    "            date.append(date_text)\r\n",
    "\r\n",
    "        page_num += 1\r\n",
    "        print(\"Page Switched\")\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        print(e)\r\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://wuzzuf.net/search/jobs/?a=navbl&filters%5Bcareer_level%5D%5B0%5D=Entry%20Level&filters%5Bjob_types%5D%5B0%5D=full_time&filters%5Bjob_types%5D%5B1%5D=work_from_home&filters%5Bpost_date%5D%5B0%5D=within_24_hours&q=data%20analyst&start=0\n",
      "Page Switched\n",
      "https://wuzzuf.net/search/jobs/?a=navbl&filters%5Bcareer_level%5D%5B0%5D=Entry%20Level&filters%5Bjob_types%5D%5B0%5D=full_time&filters%5Bjob_types%5D%5B1%5D=work_from_home&filters%5Bpost_date%5D%5B0%5D=within_24_hours&q=data%20analyst&start=1\n",
      "Pages ended\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#using selenium to iterate through each link and get needed info\r\n",
    "driver = webdriver.Chrome(driver_path)\r\n",
    "\r\n",
    "for link in links:\r\n",
    "    try:\r\n",
    "        #getting each info by it's x_path\r\n",
    "        driver.get(link)\r\n",
    "        \r\n",
    "        No_applicants = driver.find_element_by_xpath('//strong[@class = \"css-u1gwks\"]')\r\n",
    "        No_of_applicants.append(No_applicants.text)\r\n",
    "        \r\n",
    "        experiance = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][1]/span[2]/span')\r\n",
    "        Experience_Needed.append(experiance.text)\r\n",
    "        \r\n",
    "        careerlevel = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][2]/span[2]/span')\r\n",
    "        Career_Level.append(careerlevel.text)\r\n",
    "        \r\n",
    "        education = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][3]/span[2]/span')\r\n",
    "        Education_Level.append(education.text)\r\n",
    "        \r\n",
    "        #Job_Categories is more than one so we will loop through the elements of it's x_path\r\n",
    "        Jobcategories_text = \"\"\r\n",
    "        Jobcategories = driver.find_elements_by_xpath('//div[@class=\"css-13sf2ik\"]/ul/li/a/span')\r\n",
    "        for jobcat in Jobcategories:\r\n",
    "            Jobcategories_text += jobcat.text + \" | \"\r\n",
    "        Jobcategories_text  = Jobcategories_text[:-2]\r\n",
    "        Job_Categories.append(Jobcategories_text)\r\n",
    "        \r\n",
    "        #Requirements is more than one so we will loop through the elements of it's x_path\r\n",
    "        reqs_text = \"\"\r\n",
    "        reqs = driver.find_elements_by_xpath('//div[@class = \"css-1t5f0fr\"]/ul/li')\r\n",
    "        for req in reqs:\r\n",
    "            reqs_text += req.text + \" | \"\r\n",
    "        reqs_text = reqs_text[:-2]\r\n",
    "        Job_Requirements.append(reqs_text)\r\n",
    "            \r\n",
    "        salaries = driver.find_element_by_xpath('//div[@class=\"css-rcl8e5\"][4]/span[2]/span')\r\n",
    "        Salary.append(salaries.text)\r\n",
    "    except:\r\n",
    "        Experience_Needed.append(\"not found\")\r\n",
    "        Career_Level.append(\"not found\")\r\n",
    "        Education_Level.append(\"not found\")\r\n",
    "        Job_Categories.append(\"not found\")        \r\n",
    "        Salary.append(\"not found\")\r\n",
    "        print(\"one job data not found\")\r\n",
    "    continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creating csv file\r\n",
    "\r\n",
    "\r\n",
    "file_list = [job_title, company_name, location, date, Salary, \r\n",
    "             Experience_Needed, Career_Level, Education_Level, Job_Categories, links, No_of_applicants, Job_Requirements]\r\n",
    "exported = zip_longest(*file_list)\r\n",
    "\r\n",
    "with open(file_path, \"w\", encoding='utf-8') as myfile:\r\n",
    "    wr = csv.writer(myfile)\r\n",
    "    wr.writerow(['job_title', 'company_name', 'location', 'date', 'Salary', 'Experience_Needed', 'Career_Level',\r\n",
    "                 'Education_Level', 'Job_Categories', 'links', 'No_of_applicants', 'Job_Requirements'])\r\n",
    "    wr.writerows(exported)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check all lists have the same length\r\n",
    "file_list = [job_title, company_name, location, date, Salary, \r\n",
    "             Experience_Needed, Career_Level, Education_Level, Job_Categories, links, No_of_applicants, Job_Requirements]\r\n",
    "for i in (file_list):\r\n",
    "    print(len(i))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2b19cefb9f5d0dfedeb79ee08d95016c594612731e7160d4ae76104f66d4762"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('scrapping': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}